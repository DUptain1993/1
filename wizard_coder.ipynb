{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DUptain1993/1/blob/main/wizard_coder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "suppress-debugger-warning",
      "metadata": {
        "id": "suppress-debugger-warning"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
        "print(\"Debugger file validation disabled to suppress frozen modules warning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install-packages",
      "metadata": {
        "id": "install-packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers>=4.33.0 datasets accelerate peft bitsandbytes\n",
        "!pip install -q google-cloud-storage torch\n",
        "!pip install -q auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports-and-auth",
      "metadata": {
        "id": "imports-and-auth"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from google.cloud import storage\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset\n",
        "\n",
        "# --- Authenticate with GCS using Kaggle Secrets ---\n",
        "service_account_path = \"/kaggle/working/gcs_service_account.json\"\n",
        "\n",
        "if not os.path.exists(service_account_path):\n",
        "    with open(service_account_path, \"w\") as f:\n",
        "        f.write(os.environ.get(\"GCS_SERVICE_ACCOUNT\", \"\"))\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = service_account_path\n",
        "\n",
        "# Initialize GCS client and bucket\n",
        "BUCKET_NAME = 'wizardlm-training-1759276927'\n",
        "DATASET_PATH = 'datasets/combined_training_data.jsonl'\n",
        "\n",
        "client = storage.Client()\n",
        "bucket = client.bucket(BUCKET_NAME)\n",
        "print(f\"‚úÖ Connected to GCS bucket: {BUCKET_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-dataset",
      "metadata": {
        "id": "load-dataset"
      },
      "outputs": [],
      "source": [
        "def load_wizardlm_dataset():\n",
        "    blob = bucket.blob(DATASET_PATH)\n",
        "    content = blob.download_as_text()\n",
        "    data = [json.loads(line) for line in content.strip().split('\\n') if line.strip()]\n",
        "    print(f\"üìä Loaded {len(data)} WizardLM training examples\")\n",
        "    return data\n",
        "\n",
        "dataset = load_wizardlm_dataset()\n",
        "\n",
        "print(\"\\nüìù Sample WizardLM data:\")\n",
        "print(json.dumps(dataset[0], indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-model",
      "metadata": {
        "id": "load-model"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"QuixiAI/WizardLM-1.0-Uncensored-CodeLlama-34b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ WizardLM model loaded with {model.num_parameters():,} parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "apply-lora",
      "metadata": {
        "id": "apply-lora"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"‚úÖ LoRA configuration applied\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prepare-tokenize-data",
      "metadata": {
        "id": "prepare-tokenize-data"
      },
      "outputs": [],
      "source": [
        "def prepare_wizardlm_data(data, max_samples=500):\n",
        "    data = data[:max_samples]\n",
        "    training_texts = [item.get('text', '') for item in data if item.get('text', '') and len(item.get('text', '')) > 50]\n",
        "    return training_texts\n",
        "\n",
        "training_texts = prepare_wizardlm_data(dataset, max_samples=500)\n",
        "print(f\"üìù Prepared {len(training_texts)} training examples\")\n",
        "\n",
        "def tokenize_wizardlm(examples):\n",
        "    return tokenizer(\n",
        "        examples,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=1024,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "train_dataset = Dataset.from_dict({\"text\": training_texts})\n",
        "train_dataset = train_dataset.map(\n",
        "    lambda x: tokenize_wizardlm([x[\"text\"]]),\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Dataset tokenized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "training-setup",
      "metadata": {
        "id": "training-setup"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./wizardlm_fine_tuned\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    warmup_steps=50,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    report_to=None,\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training configuration ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train-model",
      "metadata": {
        "id": "train-model"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"‚úÖ Fine-tuning completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save-model",
      "metadata": {
        "id": "save-model"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "def save_wizardlm_to_gcs():\n",
        "    model.save_pretrained(\"./wizardlm_fine_tuned\")\n",
        "    tokenizer.save_pretrained(\"./wizardlm_fine_tuned\")\n",
        "\n",
        "    shutil.make_archive(\"wizardlm_fine_tuned\", \"zip\", \"./wizardlm_fine_tuned\")\n",
        "\n",
        "    blob = bucket.blob(\"model_output/wizardlm_fine_tuned_kaggle.zip\")\n",
        "    blob.upload_from_filename(\"wizardlm_fine_tuned.zip\")\n",
        "\n",
        "    print(\"‚úÖ Fine-tuned model saved to GCS at:\")\n",
        "    print(\"gs://wizardlm-training-1759276927/model_output/wizardlm_fine_tuned_kaggle.zip\")\n",
        "\n",
        "save_wizardlm_to_gcs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-model",
      "metadata": {
        "id": "test-model"
      },
      "outputs": [],
      "source": [
        "def test_wizardlm(prompt=\"### Human: Explain how to implement a neural network in Python\\n### Assistant:\"):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=inputs[\"input_ids\"].shape[1] + 200,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            top_p=0.9,\n",
        "            top_k=50\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"üßô‚Äç‚ôÇÔ∏è WizardLM Response:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {response}\")\n",
        "\n",
        "test_wizardlm()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "name": "wizard-coder",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}